# Deep Funding Analytics Challenge 
An AI method to analyze different GitHub repositories and determine a relationship score between two GitHub repositories.

# Execution
install dependencies
```bash
pip install -r requirements.txt
```

# Deep Funding Predictive Challenge Mini-Contest Description

I submitted my results for the competition as follows:  
- **Part 1**: Result named `allen0915` on Hugging Face Space  
- **Part 2**: Result named `Allen Chu` on the Pond platform  

### Public Leaderboard Scores:
- **Part 1 (MSE):** 0.0194  
- **Part 2 (MSE):** 0.0594  

#### Reference:
- **GitHub Repository**: [GitHub link]

---

### Approach and Methodology

#### Data Collection and Feature Engineering

The provided datasets (`dataset.csv` and `test.csv`) contained limited information: two GitHub project links and their relative funding weights (`weight_a + weight_b = 1`). These features alone were insufficient for building a robust machine learning model. 

To address this, I utilized the GitHub API to extract comprehensive repository metadata. The raw data was preprocessed and refined into structured features. Below are the key features engineered through this process:

- **Repository Attributes**:  
  - `is_fork`  
  - `fork_count`  
  - `star_count`  
  - `watcher_count`  
  - `language`  
  - `license_spdx_id`  
  - `repo_created_at`  
  - `repo_updated_at`  
  - `repo_exist_days` (calculated as the difference between the current date and creation date)  

- **Commit Information**:  
  - `first_commit_time`  
  - `last_commit_time`  
  - `commit_count`  
  - `work_days` (difference between the first and last commit times)  
  - `days_with_commits_count` (total days with commits)  

- **Community Engagement**:  
  - `contributors_to_repo_count`

Additionally, I employed Vision-Language Models (VLM) and web-crawling techniques to augment the dataset further. Using Large Language Models (LLMs), I created a set of qualitative metrics to evaluate repository quality. The evaluation criteria and prompts were as follows:

```json
{
    "Readme_score": "Rate the quality of the README on a scale of 1 to 5, with 5 being the best and 1 being the worst. Decimals are allowed.",
    "technical_innovation": "Rate the technical innovation of the repository on a scale of 1 to 5, with 5 being highly innovative and 1 being not innovative.",
    "community_engagement": "Rate the community engagement on a scale of 1 to 5, with 5 being highly engaged and 1 being poor engagement.",
    "accessibility": "Rate the friendliness to new contributors on a scale of 1 to 5, with 5 being very friendly and 1 being not friendly."
}
```
These features were paired to create input datasets where the information for project_a and project_b was used to predict the target label weight_a.

Solution and Model Development
For model training, I experimented with various algorithms, ranging from non-tree-based methods (Linear Regression, SVM) to tree-based ensemble methods (Random Forest, XGBoost). Among these, XGBoost outperformed the others in MSE score.

Feature Selection
To refine the model further, I analyzed feature importance scores generated by XGBoost. Below is an example of the feature importance plot:


By iteratively testing different thresholds for feature selection, I identified the top 20 most important features that minimized the MSE on the testing data. These features were then used as inputs for the final model.

### Future work
Model Optimization:
Fine-tuning XGBoost hyperparameters to achieve even lower MSE scores.
Feature Expansion:
Exploring additional GitHub repository features and testing combinations to improve predictive accuracy.
LLM Utilization:
Further integrating LLMs for qualitative evaluation of repositories. As some research papers suggest, LLMs can effectively analyze projects considering multiple complex factors beyond numeric metrics like star_count and fork_count.

This competition has been an exciting opportunity to experiment with innovative approaches for predicting funding weights and enhancing model performance.


# Current thought
Since using SVM and linear Regression get poorer result. It is obvious that it is not a linear problem.
Try using tree based model like Xgboost. 

# Reference 
### paper
https://arxiv.org/pdf/2107.05112

https://openreview.net/pdf?id=vyRAYoxUuA

### Challenge link
https://huggingface.co/spaces/DeepFunding/PredictiveFundingChallengeforOpenSourceDependencies

### Example code
https://github.com/opensource-observer/insights/blob/main/community/dependency_graph_funding/Example_WeightedGraph.ipynb
